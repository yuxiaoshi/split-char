1. attention model for NMT  
http://blog.csdn.net/malefactor/article/details/50550211


2. NLP通用优化思路
http://blog.csdn.net/malefactor/article/details/50725480


3. BIRNN
http://blog.csdn.net/jojozhangju/article/details/51982254


4. BP算法的直观理解
http://blog.csdn.net/mao_xiao_feng/article/details/53048213


5. 基于RNN的语言模型与机器翻译NMT
http://blog.csdn.net/young_gy/article/details/73412285


6. RNN学习笔记及代码
http://lib.csdn.net/article/deeplearning/45502


7. 神经机器翻译NMT学习资料整理
http://blog.csdn.net/wangxinginnlp/article/details/52944649


8. NMT介绍
https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-with-gpus/


9. Recurrent Neural Networks with Word Embeddings
http://deeplearning.net/tutorial/rnnslu.html#rnnslu


10. 爬虫解析
http://blog.csdn.net/c406495762/article/details/60137956


11. 卷积神经网络在自然语言处理中的应用
http://www.csdn.net/article/2015-11-11/2826192


12. CNN介绍博客
http://colah.github.io/posts/2014-07-Conv-Nets-Modular/


13. DL学习笔记
http://www.cnblogs.com/xialuobo/p/5863675.html


14. CS231n课程笔记翻译
https://zhuanlan.zhihu.com/p/20894041


15. 交叉熵函数的推导
http://blog.csdn.net/jasonzzj/article/details/52017438
http://blog.csdn.net/u014313009/article/details/51043064
http://cs.rochester.edu/u/james/CSC248/Lec1.pdf


16. 2017-ACL paper
Neural Word Segmentation with Rich Pretraining
Chunk-based Decoder for Neural Machine Translation


17. 维特比算法详解
http://blog.csdn.net/zb1165048017/article/details/48578183


18.机器学习详细笔记
http://www.cnblogs.com/ooon/p/4947347.html


19. 最大熵学习笔记
http://blog.csdn.net/itplus/article/details/26550201


20. 如何查阅自然语言处理（NLP）领域学术资料
http://blog.csdn.net/hejunqing14/article/details/52193304


21. NLP常用算法解
http://zhikaizhang.cn/2016/05/31/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B9%8BLSA/


22. 综合学习
http://blog.csdn.net/GarfieldEr007/article/category/6127627


23. 深度学习课程汇总
http://blog.csdn.net/dqcfkyqdxym3f8rb0/article/details/79132402


24. coursera 微积分课程
https://www.coursera.org/learn/calculus1?siteID=SAyYsTvLiGQ-0Kgcly2TsB5vp.pZHFSiDw?siteID=Gr6prw2kaB0-nrxsOQ60bLSRsOpf50y41g&utm_content=10&utm_medium=partners&utm_source=linkshare&utm_campaign=Gr6prw2kaB0


25. tf主要知识点回顾：
a. tf.app.flags.FLAGE传参数，必须启用app.run()和main(_)函数
b. 加载模型的方法： saver.restore，tf.train.import_meta_graph, tf.contrib.framework.list_variables
   https://stackoverflow.com/questions/37086268/rename-variable-scope-of-saved-model-in-tensorflow
   使用不同的graph和session加载不同graph的模型：
   http://blog.csdn.net/jmh1996/article/details/78793650
c. 同时融合多种模型的办法
d. Dense layer的意义， tf.nn.conv2d各个参数的含义
e. 学会区分tf.name_scope 和 tf.variable_scope
   http://blog.csdn.net/uestc_c2_403/article/details/72328815
f. tf.get_variable函数的使用
   http://blog.csdn.net/uestc_c2_403/article/details/72327321


26. seq2seq源码解析
http://blog.csdn.net/yezhenxu1992/article/details/72156233


27. seq2seq中attention wrapper的详解
http://blog.csdn.net/leadai/article/details/78809788


29. 破解有道翻译反爬虫机制
http://cache.baiducontent.com/c?m=9f65cb4a8c8507ed19fa950d100b92235c43801478d7875f68d4e419ce3b46121824fefd3a231500d6c67e7344f2090ae5e74775207222a0ebc99f3dd9ac935838fe2623061e913115c468addc4755d650954d9ad90e91bae745e3b9d3a38c1608&p=81759a45d5c42deb34a7c7710f528a&newp=906cde0186cc41ac5facc7710f448e231610db2151d7d701298ffe0cc4241a1a1a3aecbf20231701d3ce776501aa4858ebf53178310034f1f689df08d2ecce7e6e&user=baidu&fm=sc&query=%C5%C0%B3%E6%D3%D0%B5%C0%B7%AD%D2%EBapi&qid=9124f0210002a20d&p1=8




