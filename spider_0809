# -*- coding: utf-8 -*-
"""
Created on Wed Aug  9 14:35:36 2017

"""

from urllib import request, parse
import time, random
from bs4 import BeautifulSoup

Headers = {
        'Host': 'chinese.donga.com',
        'Proxy-Connection': 'Keep-Alive',
        'Cache-Control':'max-age=0',
        'Upgrade-Insecure-Requests':'1',
        'User-Agent':'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',
        'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language':'zh-CN,zh;q=0.8'
        }

link_list = []
title_dict = {}
text_dict = {}

def GetHtml(url):
    a = 1
    #http://chinese.donga.com/List?p0=j&p=1&l=20&c=02
    url = 'http://chinese.donga.com/List?p0=j&p={}&l=20&c=02'.format(a)
    proxy = request.ProxyHandler({'http':'109.105.1.52:8080'})
    opener = request.build_opener(proxy)
    request.install_opener(opener)
    req = request.Request(url, headers = Headers)
    page = request.urlopen(req, timeout = 10)
    html = page.read()
    
    soup = BeautifulSoup(html)
    link_list = [e.a['href'] for e in soup.body.find_all('span',class_='text')]
        
    a += 20
        

def GetText(link_list):
    
    while len(link_list) > 0 :
        link = link_list.pop()
        
        kor_url = link + '?m=kor'
        chn_url = link + '?m=chn'
        
        proxy = request.ProxyHandler({'http':'109.105.1.52:8080'})
        opener = request.build_opener(proxy)
        request.install_opener(opener)
        
        req1 = request.Request(kor_url, headers = Headers)
        page1 = request.urlopen(req1, timeout = 10)
        html1 = page1.read()
        soup1 = BeautifulSoup(html1)
        
        kor_title = soup1.find('div',id = 'view_title').h3.string
        kor_text1 = soup1.find('div',class_='news_view').find_all('p')
        
        req2 = request.Request(chn_url, headers = Headers)
        page2 = request.urlopen(req2, timeout = 10)
        html2 = page2.read()
        soup2 = BeautifulSoup(html2)
        
        chn_title = soup2.find('div',id = 'view_title').h3.string
        chn_text1 = soup2.find('div',class_='news_view').find_all('p')
        
        kor_text = []
        chn_text = []
        
        for idx,item1 in enumerate(kor_text1):
            if item1.string is not None:
                if item1.string.strip() not in (''):
                    kor_text.append(item1.string.strip())
            
        for idx,item2 in enumerate(chn_text1):
            if item2.string is not None:
                if item2.string.strip() not in (''):
                    chn_text.append(item2.string.strip())

        for item1,item2 in zip(kor_text,chn_text):
            text_dict[item1] = item2
        title_dict[kor_title] = chn_title
        
        time.sleep(1*random.random()+1)
            
        


        
    
    
    
    
    
    
    
    
    
    
